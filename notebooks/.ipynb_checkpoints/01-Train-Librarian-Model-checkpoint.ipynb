{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4e5e13-e1de-4811-a3ed-3d80152e97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "--- Step 0: Ensuring Correct Libraries ---\n",
      "======================================================================\n",
      "Requirement already satisfied: spacy==3.7.2 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: typer<0.10.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (0.9.4)\n",
      "Requirement already satisfied: click<8.2.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (8.1.8)\n",
      "Requirement already satisfied: tqdm in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (0.3.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from spacy==3.7.2) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from typer<0.10.0) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2025.11.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.1.5)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages (from jinja2->spacy==3.7.2) (3.0.3)\n",
      "‚úì Libraries installed successfully\n",
      "\n",
      "======================================================================\n",
      "--- Step 1: Loading and Cleaning Data ---\n",
      "======================================================================\n",
      "Found 75 tasks in annotations.json\n",
      "\n",
      "‚úì Task 1: 3 entities found\n",
      "‚úì Task 2: 6 entities found\n",
      "‚úì Task 3: 4 entities found\n",
      "‚úì Task 4: 4 entities found\n",
      "‚úì Task 5: 7 entities found\n",
      "‚úì Task 6: 3 entities found\n",
      "‚úì Task 7: 6 entities found\n",
      "‚úì Task 8: 8 entities found\n",
      "‚úì Task 9: 5 entities found\n",
      "‚úì Task 10: 6 entities found\n",
      "‚úì Task 11: 6 entities found\n",
      "‚úì Task 12: 3 entities found\n",
      "‚úì Task 13: 3 entities found\n",
      "‚úì Task 14: 3 entities found\n",
      "‚úì Task 15: 4 entities found\n",
      "‚úì Task 16: 6 entities found\n",
      "‚úì Task 17: 4 entities found\n",
      "‚úì Task 18: 6 entities found\n",
      "‚úì Task 19: 5 entities found\n",
      "‚úì Task 20: 5 entities found\n",
      "‚úì Task 21: 5 entities found\n",
      "‚úì Task 22: 3 entities found\n",
      "‚úì Task 23: 6 entities found\n",
      "‚úì Task 24: 3 entities found\n",
      "‚úì Task 25: 6 entities found\n",
      "‚úì Task 26: 5 entities found\n",
      "‚úì Task 27: 4 entities found\n",
      "‚úì Task 28: 5 entities found\n",
      "‚úì Task 29: 7 entities found\n",
      "‚úì Task 30: 5 entities found\n",
      "‚úì Task 31: 5 entities found\n",
      "‚úì Task 32: 4 entities found\n",
      "‚úì Task 33: 4 entities found\n",
      "‚úì Task 34: 4 entities found\n",
      "‚úì Task 35: 6 entities found\n",
      "‚úì Task 36: 4 entities found\n",
      "‚úì Task 37: 3 entities found\n",
      "‚úì Task 38: 6 entities found\n",
      "‚úì Task 39: 3 entities found\n",
      "‚úì Task 40: 3 entities found\n",
      "‚úì Task 41: 7 entities found\n",
      "‚úì Task 42: 3 entities found\n",
      "‚úì Task 43: 4 entities found\n",
      "‚úì Task 44: 4 entities found\n",
      "‚úì Task 45: 3 entities found\n",
      "‚úì Task 46: 5 entities found\n",
      "‚úì Task 47: 4 entities found\n",
      "‚úì Task 48: 5 entities found\n",
      "‚úì Task 49: 3 entities found\n",
      "‚úì Task 50: 6 entities found\n",
      "‚úì Task 51: 2 entities found\n",
      "‚úì Task 52: 3 entities found\n",
      "‚úì Task 53: 5 entities found\n",
      "‚úì Task 54: 3 entities found\n",
      "‚úì Task 55: 4 entities found\n",
      "‚úì Task 56: 5 entities found\n",
      "‚úì Task 57: 5 entities found\n",
      "‚úì Task 58: 6 entities found\n",
      "‚úì Task 59: 4 entities found\n",
      "‚úì Task 60: 4 entities found\n",
      "‚úì Task 61: 5 entities found\n",
      "‚úì Task 62: 5 entities found\n",
      "‚úì Task 63: 5 entities found\n",
      "‚úì Task 64: 3 entities found\n",
      "‚úì Task 65: 4 entities found\n",
      "‚úì Task 66: 7 entities found\n",
      "‚úì Task 67: 3 entities found\n",
      "‚úì Task 68: 5 entities found\n",
      "‚úì Task 69: 6 entities found\n",
      "‚úì Task 70: 4 entities found\n",
      "‚úì Task 71: 3 entities found\n",
      "‚úì Task 72: 4 entities found\n",
      "‚úì Task 73: 4 entities found\n",
      "‚úì Task 74: 3 entities found\n",
      "‚úì Task 75: 4 entities found\n",
      "\n",
      "‚úì Successfully loaded 75 documents with annotations\n",
      "\n",
      "======================================================================\n",
      "--- Step 2: Creating .spacy Files ---\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 60 documents\n",
      "Valid set: 15 documents\n",
      "\n",
      "Converting Training documents to spaCy format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 166.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Validation documents to spaCy format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 169.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Saved train.spacy (60 documents)\n",
      "‚úì Saved valid.spacy (15 documents)\n",
      "\n",
      "======================================================================\n",
      "--- Step 2.5: Creating spaCy Configuration File ---\n",
      "======================================================================\n",
      "‚úì Config file created at ./training/config.cfg\n",
      "\n",
      "======================================================================\n",
      "--- Step 3: Training NER Model (Persian & English) ---\n",
      "======================================================================\n",
      "Output directory: ../models/librarian_ner_model_v1\n",
      "\n",
      "Starting training (this may take several minutes)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devkian/miniconda3/envs/bedrock_env/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Saving to output directory: ../models/librarian_ner_model_v1\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Pipeline: ['ner']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  --------  ------  ------  ------  ------\n",
      "  0       0    173.71    0.00    0.00    0.00    0.00\n",
      "\u001b[38;5;2m‚úî Saved pipeline to output directory\u001b[0m\n",
      "../models/librarian_ner_model_v1/model-last\n",
      "\n",
      "‚úì Training completed successfully!\n",
      "\n",
      "======================================================================\n",
      "--- Step 4: Evaluating Model on Validation Set ---\n",
      "======================================================================\n",
      "Loading model from: ../models/librarian_ner_model_v1/model-best\n",
      "\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "Creating evaluation examples...\n",
      "\n",
      "Scoring model on validation set...\n",
      "\n",
      "======================================================================\n",
      "--- EVALUATION RESULTS ---\n",
      "======================================================================\n",
      "\n",
      "üìä Overall F1-Score: 0.0000\n",
      "   Precision: 0.0000\n",
      "   Recall: 0.0000\n",
      "\n",
      "üìã Scores per Entity Type:\n",
      "----------------------------------------------------------------------\n",
      "  AUTHOR               | F1: 0.0000 | P: 0.0000 | R: 0.0000\n",
      "  PUBLICATION_DATE     | F1: 0.0000 | P: 0.0000 | R: 0.0000\n",
      "  TITLE                | F1: 0.0000 | P: 0.0000 | R: 0.0000\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úì Validation set size: 15 documents\n",
      "‚úì Training set size: 60 documents\n",
      "\n",
      "======================================================================\n",
      "‚úì TRAINING AND EVALUATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Model saved at: ../models/librarian_ner_model_v1\n",
      "Best model at: ../models/librarian_ner_model_v1/model-best\n",
      "\n",
      "You can now use the model with:\n",
      "  nlp = spacy.load('../models/librarian_ner_model_v1/model-best')\n",
      "  doc = nlp('your text here')\n",
      "  for ent in doc.ents:\n",
      "      print(ent.text, ent.label_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete spaCy NER Training - OPTIMIZED FOR SMALL DATASET\n",
    "# Fixed training parameters for 75 documents\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.scorer import Scorer\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"--- Step 0: Ensuring Correct Libraries ---\")\n",
    "print(\"=\" * 70)\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', \n",
    "                          'spacy==3.7.2', 'typer<0.10.0', 'click<8.2.0', 'tqdm'])\n",
    "    print(\"‚úì Libraries installed successfully\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Warning: Could not install libraries. Error: {e}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"--- Step 1: Loading and Cleaning Data ---\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ANNOTATIONS_PATH = \"../training_data/annotations.json\"\n",
    "SOURCE_FILES_DIR = \"../training_data/source_files/\"\n",
    "training_data = []\n",
    "\n",
    "if not os.path.exists(ANNOTATIONS_PATH):\n",
    "    print(f\"‚ùå ERROR: annotations.json not found at {ANNOTATIONS_PATH}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not os.path.exists(SOURCE_FILES_DIR):\n",
    "    print(f\"‚ùå ERROR: source_files directory not found at {SOURCE_FILES_DIR}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with open(ANNOTATIONS_PATH, 'r', encoding='utf-8') as f:\n",
    "    tasks = json.load(f)\n",
    "\n",
    "print(f\"Found {len(tasks)} tasks in annotations.json\\n\")\n",
    "\n",
    "for task in tasks:\n",
    "    task_id = task.get(\"id\")\n",
    "    if task_id is None:\n",
    "        continue\n",
    "    \n",
    "    original_filename = f\"{task_id}.txt\"\n",
    "    text_filepath = os.path.join(SOURCE_FILES_DIR, original_filename)\n",
    "    \n",
    "    if not os.path.exists(text_filepath):\n",
    "        print(f\"‚ö† Skipping task {task_id}: {original_filename} not found\")\n",
    "        continue\n",
    "    \n",
    "    with open(text_filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    annotations = []\n",
    "    if task.get('annotations') and len(task['annotations']) > 0:\n",
    "        if task['annotations'][0].get('result'):\n",
    "            for entity in task['annotations'][0]['result']:\n",
    "                if entity.get('type') == 'labels':\n",
    "                    vals = entity['value']\n",
    "                    annotations.append({\n",
    "                        \"start\": vals['start'],\n",
    "                        \"end\": vals['end'],\n",
    "                        \"label\": vals['labels'][0]\n",
    "                    })\n",
    "    \n",
    "    if annotations:\n",
    "        training_data.append({\n",
    "            \"text\": text,\n",
    "            \"entities\": annotations\n",
    "        })\n",
    "        print(f\"‚úì Task {task_id}: {len(annotations)} entities found\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully loaded {len(training_data)} documents with annotations\\n\")\n",
    "\n",
    "if len(training_data) == 0:\n",
    "    print(\"‚ùå ERROR: No training data loaded!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"--- Step 2: Creating .spacy Files ---\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nlp = spacy.blank(\"xx\")\n",
    "db_train = DocBin()\n",
    "db_valid = DocBin()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(training_data)\n",
    "split_point = int(len(training_data) * 0.8)\n",
    "train_set = training_data[:split_point]\n",
    "valid_set = training_data[split_point:]\n",
    "\n",
    "print(f\"Train set: {len(train_set)} documents\")\n",
    "print(f\"Valid set: {len(valid_set)} documents\\n\")\n",
    "\n",
    "def create_spacy_docs(data_set, set_name=\"\"):\n",
    "    docs = []\n",
    "    print(f\"Converting {set_name} documents to spaCy format...\")\n",
    "    \n",
    "    for item in tqdm(data_set, desc=set_name):\n",
    "        doc = nlp.make_doc(item['text'])\n",
    "        ents = []\n",
    "        \n",
    "        for entity in item['entities']:\n",
    "            span = doc.char_span(\n",
    "                entity['start'],\n",
    "                entity['end'],\n",
    "                label=entity['label'],\n",
    "                alignment_mode=\"contract\"\n",
    "            )\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "        \n",
    "        doc.ents = filter_spans(ents)\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "train_docs = create_spacy_docs(train_set, \"Training\")\n",
    "valid_docs = create_spacy_docs(valid_set, \"Validation\")\n",
    "\n",
    "# Add docs to DocBin\n",
    "for doc in train_docs:\n",
    "    db_train.add(doc)\n",
    "\n",
    "for doc in valid_docs:\n",
    "    db_valid.add(doc)\n",
    "\n",
    "os.makedirs(\"./training\", exist_ok=True)\n",
    "db_train.to_disk(\"./training/train.spacy\")\n",
    "db_valid.to_disk(\"./training/valid.spacy\")\n",
    "\n",
    "print(f\"\\n‚úì Saved train.spacy ({len(train_docs)} documents)\")\n",
    "print(f\"‚úì Saved valid.spacy ({len(valid_docs)} documents)\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"--- Step 2.5: Creating spaCy Configuration File ---\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONFIG_PATH = \"./training/config.cfg\"\n",
    "\n",
    "# OPTIMIZED CONFIG FOR SMALL DATASET (75 docs)\n",
    "# Key changes:\n",
    "# - max_epochs = 100 (more training time)\n",
    "# - patience = 1600 (don't stop early)\n",
    "# - eval_frequency = 50 (evaluate more often)\n",
    "# - batcher with smaller batch size\n",
    "config_content = \"\"\"[paths]\n",
    "train = null\n",
    "dev = null\n",
    "\n",
    "[system]\n",
    "seed = 0\n",
    "gpu_allocator = null\n",
    "allow_alloc_mb = 5000\n",
    "\n",
    "[nlp]\n",
    "lang = \"xx\"\n",
    "pipeline = [\"ner\"]\n",
    "disabled = []\n",
    "batch_size = 128\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "moves = null\n",
    "update_with_oracle_cut_size = 0\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "state_type = \"ner\"\n",
    "extra_state_tokens = false\n",
    "hidden_width = 64\n",
    "maxout_pieces = 2\n",
    "use_upper = true\n",
    "\n",
    "[components.ner.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.ner.model.tok2vec.embed]\n",
    "@architectures = \"spacy.CharacterEmbed.v2\"\n",
    "width = 96\n",
    "rows = 7000\n",
    "nM = 64\n",
    "nC = 8\n",
    "include_static_vectors = false\n",
    "\n",
    "[components.ner.model.tok2vec.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[training]\n",
    "dev_corpus = \"corpora.dev\"\n",
    "train_corpus = \"corpora.train\"\n",
    "seed = ${system.seed}\n",
    "gpu_allocator = ${system.gpu_allocator}\n",
    "dropout = 0.1\n",
    "max_epochs = 100\n",
    "patience = 1600\n",
    "max_steps = 0\n",
    "eval_frequency = 50\n",
    "accumulate_gradient = 1\n",
    "score_weights = {\"ents_f\": 1.0, \"ents_p\": 0.0, \"ents_r\": 0.0}\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "size = 1000\n",
    "discard_oversize = false\n",
    "tolerance = 0.2\n",
    "\n",
    "[training.logger]\n",
    "@loggers = \"spacy.ConsoleLogger.v1\"\n",
    "progress_bar = true\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "\n",
    "[training.optimizer.learn_rate]\n",
    "@schedules = \"warmup_linear.v1\"\n",
    "warmup_steps = 250\n",
    "total_steps = 20000\n",
    "initial_rate = 0.001\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[corpora.train]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${paths.train}\n",
    "max_length = 0\n",
    "gold_preproc = false\n",
    "limit = 0\n",
    "augmenter = null\n",
    "\n",
    "[corpora.dev]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${paths.dev}\n",
    "max_length = 0\n",
    "gold_preproc = false\n",
    "limit = 0\n",
    "augmenter = null\n",
    "\n",
    "[initialize]\n",
    "components = {}\n",
    "before_init = null\n",
    "after_init = null\n",
    "\"\"\"\n",
    "\n",
    "with open(CONFIG_PATH, 'w', encoding='utf-8') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"‚úì Config file created at {CONFIG_PATH}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"--- Step 3: Training NER Model (Persian & English) ---\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "OUTPUT_DIR = \"../models/librarian_ner_model_v1\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_cmd = [\n",
    "    sys.executable, '-m', 'spacy', 'train',\n",
    "    CONFIG_PATH,\n",
    "    '--output', OUTPUT_DIR,\n",
    "    '--paths.train', './training/train.spacy',\n",
    "    '--paths.dev', './training/valid.spacy'\n",
    "]\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\\n\")\n",
    "print(\"Starting training (this may take 5-15 minutes)...\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(train_cmd, check=True, capture_output=False)\n",
    "    print(\"\\n‚úì Training completed successfully!\\n\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\n‚ùå Training failed with error code {e.returncode}\")\n",
    "    print(f\"Error: {e}\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"--- Step 4: Evaluating Model on Validation Set ---\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "MODEL_PATH = os.path.join(OUTPUT_DIR, \"model-best\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"‚ùå ERROR: Model not found at {MODEL_PATH}\")\n",
    "    print(\"Training may have failed. Check the output above.\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\\n\")\n",
    "\n",
    "try:\n",
    "    nlp_custom = spacy.load(MODEL_PATH)\n",
    "    print(f\"‚úì Model loaded successfully\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load model: {e}\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Creating evaluation examples...\\n\")\n",
    "eval_examples = []\n",
    "for doc in valid_docs:\n",
    "    pred_doc = nlp_custom(doc.text)\n",
    "    eval_examples.append(Example(pred_doc, doc))\n",
    "\n",
    "print(\"Scoring model on validation set...\\n\")\n",
    "scorer = Scorer()\n",
    "scores = scorer.score(eval_examples)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"--- EVALUATION RESULTS ---\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if scores:\n",
    "    print(f\"\\nüìä Overall F1-Score: {scores.get('ents_f', 0):.4f}\")\n",
    "    print(f\"   Precision: {scores.get('ents_p', 0):.4f}\")\n",
    "    print(f\"   Recall: {scores.get('ents_r', 0):.4f}\")\n",
    "    \n",
    "    if scores.get('ents_per_type'):\n",
    "        print(\"\\nüìã Scores per Entity Type:\")\n",
    "        print(\"-\" * 70)\n",
    "        for label, metrics in sorted(scores['ents_per_type'].items()):\n",
    "            f_score = metrics.get('f', 0)\n",
    "            precision = metrics.get('p', 0)\n",
    "            recall = metrics.get('r', 0)\n",
    "            print(f\"  {label.upper():20} | F1: {f_score:.4f} | P: {precision:.4f} | R: {recall:.4f}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    print(f\"\\n‚úì Validation set size: {len(eval_examples)} documents\")\n",
    "    print(f\"‚úì Training set size: {len(train_docs)} documents\")\n",
    "else:\n",
    "    print(\"‚ùå No scores returned from evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì TRAINING AND EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel saved at: {OUTPUT_DIR}\")\n",
    "print(f\"Best model at: {MODEL_PATH}\")\n",
    "print(\"\\nYou can now use the model with:\")\n",
    "print(f\"  nlp = spacy.load('{MODEL_PATH}')\")\n",
    "print(f\"  doc = nlp('your text here')\")\n",
    "print(f\"  for ent in doc.ents:\")\n",
    "print(f\"      print(ent.text, ent.label_)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676c1d3-91de-41df-a2e1-f383216054c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Bedrock)",
   "language": "python",
   "name": "bedrock_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
